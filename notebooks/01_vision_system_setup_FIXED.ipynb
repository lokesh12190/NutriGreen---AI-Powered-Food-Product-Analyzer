{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ Notebook 1: Three-Tier Vision System Setup (CORRECTED FOR 6GB GPU)\n",
    "\n",
    "## ‚ö†Ô∏è IMPORTANT: VRAM Management for RTX 3060 6GB\n",
    "\n",
    "Your GPU has **6GB VRAM total**, but:\n",
    "- **Moondream2** needs ~3.5 GB\n",
    "- **LLaVA-1.5** needs ~4-5 GB\n",
    "- **Both together** need ~7-8 GB ‚ùå (won't fit!)\n",
    "\n",
    "**Solution:** We'll use a **dynamic loading** strategy - load models on-demand and clear them when switching.\n",
    "\n",
    "## What This Notebook Does\n",
    "1. ‚úÖ Install required libraries\n",
    "2. ‚úÖ Test GPU availability\n",
    "3. ‚úÖ Setup dynamic model loading (one at a time)\n",
    "4. ‚úÖ Load API key from .env file\n",
    "5. ‚úÖ Test all three modes\n",
    "6. ‚úÖ Export as reusable module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üì¶ Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All packages installed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: '#': Expected package name at the start of dependency specifier\n",
      "    #\n",
      "    ^\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "#!pip install -q transformers>=4.37.0\n",
    "#!pip install -q accelerate>=0.26.0\n",
    "#!pip install -q bitsandbytes>=0.42.0\n",
    "#!pip install -q einops\n",
    "#!pip install -q sentencepiece\n",
    "#!pip install -q protobuf\n",
    "#!pip install -q Pillow\n",
    "#!pip install -q openai\n",
    "!pip install -q python-dotenv  # For loading .env file\n",
    "\n",
    "print(\"‚úÖ All packages installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q python-dotenv "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üñ•Ô∏è Step 2: Verify GPU Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "GPU CONFIGURATION CHECK\n",
      "============================================================\n",
      "\n",
      "üîß CUDA Available: True\n",
      "üéÆ GPU Name: NVIDIA GeForce RTX 3060 Laptop GPU\n",
      "üíæ Total VRAM: 6.00 GB\n",
      "üî¢ CUDA Version: 12.1\n",
      "üêç PyTorch Version: 2.5.1\n",
      "\n",
      "üìä Free VRAM: 6.00 GB\n",
      "\n",
      "‚ö†Ô∏è Note: Your GPU has less than 8GB VRAM.\n",
      "   We'll use DYNAMIC LOADING - models load on-demand.\n",
      "\n",
      "‚úÖ GPU ready for dynamic model loading!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"GPU CONFIGURATION CHECK\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüîß CUDA Available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üéÆ GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    total_vram = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"üíæ Total VRAM: {total_vram:.2f} GB\")\n",
    "    print(f\"üî¢ CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"üêç PyTorch Version: {torch.__version__}\")\n",
    "    \n",
    "    free_vram = (torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated(0)) / 1024**3\n",
    "    print(f\"\\nüìä Free VRAM: {free_vram:.2f} GB\")\n",
    "    \n",
    "    if total_vram < 8:\n",
    "        print(\"\\n‚ö†Ô∏è Note: Your GPU has less than 8GB VRAM.\")\n",
    "        print(\"   We'll use DYNAMIC LOADING - models load on-demand.\")\n",
    "    \n",
    "    print(\"\\n‚úÖ GPU ready for dynamic model loading!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è WARNING: No GPU detected. Models will run on CPU (very slow)\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üîë Step 3: Load Environment Variables (.env file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ OpenAI API Key loaded from .env file\n",
      "   Key starts with: sk-proj-iv...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load .env file from current directory\n",
    "load_dotenv()\n",
    "\n",
    "# Check if OpenAI API key is loaded\n",
    "openai_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if openai_key:\n",
    "    print(\"‚úÖ OpenAI API Key loaded from .env file\")\n",
    "    print(f\"   Key starts with: {openai_key[:10]}...\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No OPENAI_API_KEY found in .env file\")\n",
    "    print(\"\\nMake sure your .env file contains:\")\n",
    "    print(\"   OPENAI_API_KEY=sk-...\")\n",
    "    print(\"\\nOr set it manually:\")\n",
    "    print(\"   os.environ['OPENAI_API_KEY'] = 'sk-...'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéØ Step 4: Create Dynamic Vision Manager (VRAM-Optimized)\n",
    "\n",
    "This version loads models **on-demand** and clears them when switching modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß VisionManager initialized (Dynamic Loading Mode)\n",
      "   Premium Mode: ‚úÖ Ready\n",
      "\n",
      "üí° Models will load on-demand to save VRAM\n",
      "\n",
      "‚úÖ VisionManager created successfully!\n",
      "\n",
      "üìä Status:\n",
      "   ‚úÖ Quick: (will load on-demand)\n",
      "   ‚úÖ Standard: (will load on-demand)\n",
      "   ‚úÖ Premium: (will load on-demand)\n",
      "   Currently loaded: None\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import io\n",
    "import base64\n",
    "import json\n",
    "import re\n",
    "from PIL import Image\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer,\n",
    "    LlavaNextProcessor,\n",
    "    LlavaNextForConditionalGeneration,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from openai import OpenAI, APIError\n",
    "\n",
    "class VisionManager:\n",
    "    \"\"\"\n",
    "    Dynamic Vision Manager - Optimized for 6GB VRAM\n",
    "    \n",
    "    Models are loaded on-demand and cleared when switching modes.\n",
    "    This allows working with limited VRAM.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, openai_api_key=None):\n",
    "        \"\"\"\n",
    "        Initialize VisionManager\n",
    "        \n",
    "        Args:\n",
    "            openai_api_key: Optional OpenAI API key for premium mode\n",
    "        \"\"\"\n",
    "        self.moondream_model = None\n",
    "        self.moondream_tokenizer = None\n",
    "        self.llava_model = None\n",
    "        self.llava_processor = None\n",
    "        self.openai_client = None\n",
    "        \n",
    "        # Get API key from parameter or environment\n",
    "        self.openai_api_key = openai_api_key or os.getenv('OPENAI_API_KEY')\n",
    "        \n",
    "        # Track currently loaded model\n",
    "        self.current_model = None\n",
    "        \n",
    "        print(\"üîß VisionManager initialized (Dynamic Loading Mode)\")\n",
    "        print(f\"   Premium Mode: {'‚úÖ Ready' if self.openai_api_key else '‚ùå No API key'}\")\n",
    "        print(\"\\nüí° Models will load on-demand to save VRAM\")\n",
    "    \n",
    "    def clear_all_models(self):\n",
    "        \"\"\"Clear all loaded models from VRAM\"\"\"\n",
    "        if self.moondream_model is not None:\n",
    "            del self.moondream_model\n",
    "            del self.moondream_tokenizer\n",
    "            self.moondream_model = None\n",
    "            self.moondream_tokenizer = None\n",
    "            print(\"üßπ Cleared Moondream from VRAM\")\n",
    "        \n",
    "        if self.llava_model is not None:\n",
    "            del self.llava_model\n",
    "            del self.llava_processor\n",
    "            self.llava_model = None\n",
    "            self.llava_processor = None\n",
    "            print(\"üßπ Cleared LLaVA from VRAM\")\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            free_vram = (torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated(0)) / 1024**3\n",
    "            print(f\"üíæ Free VRAM: {free_vram:.2f} GB\")\n",
    "        \n",
    "        self.current_model = None\n",
    "    \n",
    "    def load_moondream(self):\n",
    "        \"\"\"Load Moondream2 model\"\"\"\n",
    "        if self.moondream_model is not None:\n",
    "            print(\"‚úÖ Moondream already loaded\")\n",
    "            return True\n",
    "        \n",
    "        try:\n",
    "            print(\"\\nüì• Loading Moondream2...\")\n",
    "            \n",
    "            # Clear other models first\n",
    "            if self.current_model == \"llava\":\n",
    "                print(\"   Clearing LLaVA to make room...\")\n",
    "                if self.llava_model is not None:\n",
    "                    del self.llava_model\n",
    "                    del self.llava_processor\n",
    "                    self.llava_model = None\n",
    "                    self.llava_processor = None\n",
    "                    torch.cuda.empty_cache()\n",
    "            \n",
    "            self.moondream_model = AutoModelForCausalLM.from_pretrained(\n",
    "                \"vikhyatk/moondream2\",\n",
    "                revision=\"2024-08-26\",\n",
    "                trust_remote_code=True,\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map=\"auto\" if torch.cuda.is_available() else \"cpu\"\n",
    "            )\n",
    "            \n",
    "            self.moondream_tokenizer = AutoTokenizer.from_pretrained(\n",
    "                \"vikhyatk/moondream2\",\n",
    "                revision=\"2024-08-26\",\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            \n",
    "            self.current_model = \"moondream\"\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                vram_used = torch.cuda.memory_allocated(0) / 1024**3\n",
    "                print(f\"‚úÖ Moondream loaded - VRAM used: {vram_used:.2f} GB\")\n",
    "            else:\n",
    "                print(\"‚úÖ Moondream loaded on CPU\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading Moondream: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def load_llava(self):\n",
    "        \"\"\"Load LLaVA-1.5 model with 4-bit quantization\"\"\"\n",
    "        if self.llava_model is not None:\n",
    "            print(\"‚úÖ LLaVA already loaded\")\n",
    "            return True\n",
    "        \n",
    "        try:\n",
    "            print(\"\\nüì• Loading LLaVA-1.5 (4-bit quantization)...\")\n",
    "            \n",
    "            # Clear other models first\n",
    "            if self.current_model == \"moondream\":\n",
    "                print(\"   Clearing Moondream to make room...\")\n",
    "                if self.moondream_model is not None:\n",
    "                    del self.moondream_model\n",
    "                    del self.moondream_tokenizer\n",
    "                    self.moondream_model = None\n",
    "                    self.moondream_tokenizer = None\n",
    "                    torch.cuda.empty_cache()\n",
    "            \n",
    "            # Check available VRAM\n",
    "            if torch.cuda.is_available():\n",
    "                free_vram = (torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated(0)) / 1024**3\n",
    "                print(f\"   Free VRAM: {free_vram:.2f} GB\")\n",
    "                \n",
    "                if free_vram < 4.0:\n",
    "                    print(\"\\n‚ö†Ô∏è Not enough free VRAM for LLaVA (needs ~4GB)\")\n",
    "                    print(\"   Trying with CPU offloading...\")\n",
    "            \n",
    "            # 4-bit quantization config\n",
    "            quantization_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_compute_dtype=torch.float16,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                llm_int8_enable_fp32_cpu_offload=True  # Enable CPU offloading\n",
    "            )\n",
    "            \n",
    "            self.llava_processor = LlavaNextProcessor.from_pretrained(\n",
    "                \"llava-hf/llava-v1.6-mistral-7b-hf\"\n",
    "            )\n",
    "            \n",
    "            self.llava_model = LlavaNextForConditionalGeneration.from_pretrained(\n",
    "                \"llava-hf/llava-v1.6-mistral-7b-hf\",\n",
    "                quantization_config=quantization_config,\n",
    "                device_map=\"auto\",\n",
    "                torch_dtype=torch.float16,\n",
    "                low_cpu_mem_usage=True\n",
    "            )\n",
    "            \n",
    "            self.current_model = \"llava\"\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                vram_used = torch.cuda.memory_allocated(0) / 1024**3\n",
    "                print(f\"‚úÖ LLaVA loaded - VRAM used: {vram_used:.2f} GB\")\n",
    "            else:\n",
    "                print(\"‚úÖ LLaVA loaded on CPU\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading LLaVA: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return False\n",
    "    \n",
    "    def analyze_image(self, image, mode=\"quick\", yolo_detections=None, ocr_results=None):\n",
    "        \"\"\"\n",
    "        Analyze image with selected mode\n",
    "        \n",
    "        Args:\n",
    "            image: PIL Image or path to image\n",
    "            mode: 'quick', 'standard', or 'premium'\n",
    "            yolo_detections: List of YOLO detections\n",
    "            ocr_results: OCR results\n",
    "        \n",
    "        Returns:\n",
    "            dict: Analysis results\n",
    "        \"\"\"\n",
    "        # Load image if path\n",
    "        if isinstance(image, str):\n",
    "            image = Image.open(image)\n",
    "        \n",
    "        # Build context from YOLO and OCR\n",
    "        context = self._build_context(yolo_detections, ocr_results)\n",
    "        \n",
    "        # Route to appropriate mode\n",
    "        if mode == \"quick\":\n",
    "            return self._analyze_quick(image, context)\n",
    "        elif mode == \"standard\":\n",
    "            return self._analyze_standard(image, context)\n",
    "        elif mode == \"premium\":\n",
    "            return self._analyze_premium(image, context)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid mode: {mode}\")\n",
    "    \n",
    "    def _build_context(self, yolo_detections, ocr_results):\n",
    "        \"\"\"Build context from YOLO and OCR results\"\"\"\n",
    "        context = {}\n",
    "        \n",
    "        if yolo_detections:\n",
    "            labels = [d.get('label', '') for d in yolo_detections]\n",
    "            context['detected_labels'] = ', '.join(labels)\n",
    "        \n",
    "        if ocr_results:\n",
    "            context['ocr_text'] = ocr_results.get('raw_text', '')[:500]\n",
    "            context['brand'] = ocr_results.get('brand')\n",
    "            context['product_name'] = ocr_results.get('product_name')\n",
    "        \n",
    "        return context\n",
    "    \n",
    "    def _analyze_quick(self, image, context):\n",
    "        \"\"\"Quick mode with Moondream2\"\"\"\n",
    "        # Load model if not loaded\n",
    "        if not self.load_moondream():\n",
    "            return {\"error\": \"Failed to load Moondream model\"}\n",
    "        \n",
    "        try:\n",
    "            prompt = \"Analyze this food product. \"\n",
    "            if context.get('detected_labels'):\n",
    "                prompt += f\"Labels detected: {context['detected_labels']}. \"\n",
    "            prompt += \"Provide: category, main ingredients (if visible), and key features.\"\n",
    "            \n",
    "            start_time = time.time()\n",
    "            enc_image = self.moondream_model.encode_image(image)\n",
    "            answer = self.moondream_model.answer_question(enc_image, prompt, self.moondream_tokenizer)\n",
    "            elapsed = time.time() - start_time\n",
    "            \n",
    "            return {\n",
    "                \"mode\": \"quick\",\n",
    "                \"response\": answer,\n",
    "                \"time_seconds\": round(elapsed, 2),\n",
    "                \"context_used\": context\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"Quick mode error: {str(e)}\"}\n",
    "    \n",
    "    def _analyze_standard(self, image, context):\n",
    "        \"\"\"Standard mode with LLaVA-1.5\"\"\"\n",
    "        # Load model if not loaded\n",
    "        if not self.load_llava():\n",
    "            return {\"error\": \"Failed to load LLaVA model\"}\n",
    "        \n",
    "        try:\n",
    "            prompt = \"Analyze this food product in detail. \"\n",
    "            if context.get('detected_labels'):\n",
    "                prompt += f\"Labels detected: {context['detected_labels']}. \"\n",
    "            if context.get('ocr_text'):\n",
    "                prompt += f\"Text visible: {context['ocr_text'][:200]}... \"\n",
    "            prompt += \"Provide: category, ingredients, nutritional highlights, and dietary suitability.\"\n",
    "            \n",
    "            conversation = [{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\"},\n",
    "                    {\"type\": \"text\", \"text\": prompt}\n",
    "                ]\n",
    "            }]\n",
    "            \n",
    "            prompt_text = self.llava_processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "            \n",
    "            start_time = time.time()\n",
    "            inputs = self.llava_processor(images=image, text=prompt_text, return_tensors=\"pt\")\n",
    "            \n",
    "            # Move to GPU only if available and has space\n",
    "            if torch.cuda.is_available():\n",
    "                try:\n",
    "                    inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
    "                except:\n",
    "                    print(\"‚ö†Ô∏è Moving inputs to GPU failed, using CPU\")\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                output = self.llava_model.generate(**inputs, max_new_tokens=500, do_sample=False)\n",
    "            \n",
    "            response = self.llava_processor.decode(output[0], skip_special_tokens=True)\n",
    "            \n",
    "            if \"[/INST]\" in response:\n",
    "                response = response.split(\"[/INST]\")[-1].strip()\n",
    "            \n",
    "            elapsed = time.time() - start_time\n",
    "            \n",
    "            return {\n",
    "                \"mode\": \"standard\",\n",
    "                \"response\": response,\n",
    "                \"time_seconds\": round(elapsed, 2),\n",
    "                \"context_used\": context\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            return {\"error\": f\"Standard mode error: {str(e)}\", \"traceback\": traceback.format_exc()}\n",
    "    \n",
    "    def _analyze_premium(self, image, context):\n",
    "        \"\"\"Premium mode with OpenAI GPT-4o\"\"\"\n",
    "        if not self.openai_api_key:\n",
    "            return {\"error\": \"Premium mode not available - no API key. Check your .env file.\"}\n",
    "        \n",
    "        try:\n",
    "            # Initialize client if needed\n",
    "            if not self.openai_client:\n",
    "                self.openai_client = OpenAI(api_key=self.openai_api_key)\n",
    "            \n",
    "            # Encode image\n",
    "            buffered = io.BytesIO()\n",
    "            image.save(buffered, format=\"JPEG\")\n",
    "            image_data = base64.b64encode(buffered.getvalue()).decode('utf-8')\n",
    "            \n",
    "            prompt = \"Analyze this food product and provide structured information.\\n\\n\"\n",
    "            if context.get('detected_labels'):\n",
    "                prompt += f\"Labels detected: {context['detected_labels']}\\n\"\n",
    "            if context.get('ocr_text'):\n",
    "                prompt += f\"Text visible: {context['ocr_text'][:300]}\\n\"\n",
    "            \n",
    "            prompt += \"\"\"\\nRespond ONLY with a JSON object (no markdown) with this structure:\n",
    "{\n",
    "  \"category\": \"specific food category\",\n",
    "  \"product_type\": \"brief description\",\n",
    "  \"description\": \"2-3 sentence description\",\n",
    "  \"key_ingredients\": [\"list of main ingredients\"],\n",
    "  \"usage_suggestions\": \"how to use this product\",\n",
    "  \"suitable_for\": [\"dietary types\"]\n",
    "}\"\"\"\n",
    "            \n",
    "            start_time = time.time()\n",
    "            response = self.openai_client.chat.completions.create(\n",
    "                model=\"gpt-4o\",\n",
    "                messages=[{\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": prompt},\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\n",
    "                                \"url\": f\"data:image/jpeg;base64,{image_data}\",\n",
    "                                \"detail\": \"low\"\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                }],\n",
    "                max_tokens=800\n",
    "            )\n",
    "            \n",
    "            elapsed = time.time() - start_time\n",
    "            \n",
    "            response_text = response.choices[0].message.content\n",
    "            response_text = re.sub(r'```json\\s*|```\\s*', '', response_text).strip()\n",
    "            \n",
    "            json_match = re.search(r'\\{.*\\}', response_text, re.DOTALL)\n",
    "            \n",
    "            if json_match:\n",
    "                result = json.loads(json_match.group())\n",
    "                return {\n",
    "                    \"mode\": \"premium\",\n",
    "                    \"response\": result,\n",
    "                    \"time_seconds\": round(elapsed, 2),\n",
    "                    \"context_used\": context\n",
    "                }\n",
    "            else:\n",
    "                return {\"error\": \"No valid JSON in response\", \"raw_response\": response_text}\n",
    "            \n",
    "        except APIError as e:\n",
    "            return {\"error\": f\"OpenAI API error: {str(e)}\"}\n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"Premium mode error: {str(e)}\"}\n",
    "    \n",
    "    def get_status(self):\n",
    "        \"\"\"Get current status of all modes\"\"\"\n",
    "        return {\n",
    "            \"quick\": {\n",
    "                \"available\": True,\n",
    "                \"loaded\": self.moondream_model is not None\n",
    "            },\n",
    "            \"standard\": {\n",
    "                \"available\": True,\n",
    "                \"loaded\": self.llava_model is not None\n",
    "            },\n",
    "            \"premium\": {\n",
    "                \"available\": self.openai_api_key is not None,\n",
    "                \"loaded\": self.openai_client is not None\n",
    "            },\n",
    "            \"current_model\": self.current_model\n",
    "        }\n",
    "\n",
    "# Create global instance\n",
    "vision_manager = VisionManager()\n",
    "\n",
    "print(\"\\n‚úÖ VisionManager created successfully!\")\n",
    "print(\"\\nüìä Status:\")\n",
    "status = vision_manager.get_status()\n",
    "for mode, info in status.items():\n",
    "    if mode == \"current_model\":\n",
    "        print(f\"   Currently loaded: {info or 'None'}\")\n",
    "    else:\n",
    "        avail = \"‚úÖ\" if info['available'] else \"‚ùå\"\n",
    "        loaded = \"(loaded)\" if info['loaded'] else \"(will load on-demand)\"\n",
    "        print(f\"   {avail} {mode.capitalize()}: {loaded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üß™ Step 5: Test Quick Mode (Moondream2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì• Loading Moondream2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PhiForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From üëâv4.50üëà onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Moondream loaded - VRAM used: 3.49 GB\n",
      "‚úÖ Success! Time: 7.22s\n",
      "\n",
      "Response: The food product is a plastic container filled with a variety of fruits, including watermelon, cantaloupe, and honeydew. The container is labeled as a melon mix, which suggests that it contains multiple types of melons. The container is placed on a table, and the fruits appear to be fresh and ready to be eaten.\n",
      "‚ö†Ô∏è Update TEST_IMAGE_PATH to test Quick Mode\n"
     ]
    }
   ],
   "source": [
    "# Test Quick Mode\n",
    "TEST_IMAGE_PATH = r\"C:\\Users\\lokes\\Desktop\\ironhack\\final_project\\dataset\\dataset\\images\\nutriScoreA (812).jpg\"  # UPDATE THIS\n",
    "\n",
    "# Uncomment to test\n",
    "image = Image.open(TEST_IMAGE_PATH)\n",
    "result = vision_manager.analyze_image(image, mode=\"quick\")\n",
    "# \n",
    "if 'error' in result:\n",
    "     print(f\"‚ùå Error: {result['error']}\")\n",
    "else:\n",
    "     print(f\"‚úÖ Success! Time: {result['time_seconds']}s\")\n",
    "     print(f\"\\nResponse: {result['response']}\")\n",
    "\n",
    "print(\"‚ö†Ô∏è Update TEST_IMAGE_PATH to test Quick Mode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üß™ Step 6: Test Standard Mode (LLaVA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì• Loading LLaVA-1.5 (4-bit quantization)...\n",
      "   Clearing Moondream to make room...\n",
      "   Free VRAM: 5.99 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:38<00:00,  9.58s/it]\n",
      "c:\\anaconda\\envs\\ironhack\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\lokes\\.cache\\huggingface\\hub\\models--llava-hf--llava-v1.6-mistral-7b-hf. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LLaVA loaded - VRAM used: 4.01 GB\n",
      "‚úÖ Success! Time: 120.59s\n",
      "\n",
      "Response: Category: Fruit Salad\n",
      "\n",
      "Ingredients:\n",
      "- Watermelon\n",
      "- Cantaloupe\n",
      "- Honeydew\n",
      "- Pineapple\n",
      "- Strawberries\n",
      "\n",
      "Nutritional Highlights:\n",
      "- Rich in vitamins and minerals, particularly vitamin C, vitamin A, and potassium\n",
      "- Good source of dietary fiber\n",
      "- Low in calories and fat\n",
      "- Contains natural sugars for a quick energy boost\n",
      "\n",
      "Dietary Suitability:\n",
      "- Vegetarian and vegan-friendly\n",
      "- Suitable for those following a gluten-free diet\n",
      "- May not be suitable for individuals with allergies to certain fruits, such as pineapple or strawberries\n",
      "- It is important to note that the packaging indicates that the product contains nuts, which may be a concern for those with nut allergies.\n",
      "\n",
      "The product is a pre-cut, pre-packaged fruit salad, which is convenient for those looking for a healthy, ready-to-eat snack. The packaging also suggests that the product is suitable for pregnant women, although it is always advisable to consult with a healthcare provider for personalized dietary advice.\n",
      "‚ö†Ô∏è Update TEST_IMAGE_PATH to test Standard Mode\n"
     ]
    }
   ],
   "source": [
    "# Test Standard Mode\n",
    "# This will automatically clear Moondream and load LLaVA\n",
    "\n",
    "# Uncomment to test\n",
    "image = Image.open(TEST_IMAGE_PATH)\n",
    "result = vision_manager.analyze_image(image, mode=\"standard\")\n",
    "# \n",
    "if 'error' in result:\n",
    "     print(f\"‚ùå Error: {result['error']}\")\n",
    "else:\n",
    "     print(f\"‚úÖ Success! Time: {result['time_seconds']}s\")\n",
    "     print(f\"\\nResponse: {result['response']}\")\n",
    "\n",
    "print(\"‚ö†Ô∏è Update TEST_IMAGE_PATH to test Standard Mode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üß™ Step 7: Test Premium Mode (OpenAI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Success! Time: 5.35s\n",
      "\n",
      "Response:\n",
      "{\n",
      "  \"category\": \"fruit mix\",\n",
      "  \"product_type\": \"pre-packaged melon mix\",\n",
      "  \"description\": \"This is a ready-to-eat mix of various melon types, such as watermelon, cantaloupe, and possibly others, all cut into bite-sized pieces. It is ideal for a quick snack or as a refreshing addition to meals.\",\n",
      "  \"key_ingredients\": [\n",
      "    \"watermelon\",\n",
      "    \"cantaloupe\",\n",
      "    \"honeydew\"\n",
      "  ],\n",
      "  \"usage_suggestions\": \"Enjoy directly from the container, or add to a fruit salad or dessert.\",\n",
      "  \"suitable_for\": [\n",
      "    \"vegan\",\n",
      "    \"vegetarian\",\n",
      "    \"gluten-free\"\n",
      "  ]\n",
      "}\n",
      "‚ö†Ô∏è Update TEST_IMAGE_PATH and ensure .env has OPENAI_API_KEY\n"
     ]
    }
   ],
   "source": [
    "# Test Premium Mode\n",
    "# This uses your API key from .env file\n",
    "\n",
    "# Uncomment to test\n",
    "image = Image.open(TEST_IMAGE_PATH)\n",
    "result = vision_manager.analyze_image(image, mode=\"premium\")\n",
    "# \n",
    "if 'error' in result:\n",
    "    print(f\"‚ùå Error: {result['error']}\")\n",
    "else:\n",
    "     print(f\"‚úÖ Success! Time: {result['time_seconds']}s\")\n",
    "     print(f\"\\nResponse:\")\n",
    "     print(json.dumps(result['response'], indent=2))\n",
    "\n",
    "print(\"‚ö†Ô∏è Update TEST_IMAGE_PATH and ensure .env has OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üíæ Step 8: Export as Python Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Exported to vision_manager.py\n",
      "\n",
      "You can now use it in your Streamlit app:\n",
      "\n",
      "from vision_manager import VisionManager\n",
      "\n",
      "# Create instance (loads API key from .env automatically)\n",
      "vm = VisionManager()\n",
      "\n",
      "# Analyze image\n",
      "result = vm.analyze_image(image, mode=\"quick\")  # or \"standard\" or \"premium\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Manual export of vision_manager.py\n",
    "vision_manager_code = '''\"\"\"\n",
    "NutriGreen Vision Manager - Optimized for 6GB VRAM\n",
    "Dynamic loading: Models load on-demand and clear when switching\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "import io\n",
    "import base64\n",
    "import json\n",
    "import re\n",
    "from PIL import Image\n",
    "from dotenv import load_dotenv\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer,\n",
    "    LlavaNextProcessor,\n",
    "    LlavaNextForConditionalGeneration,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from openai import OpenAI, APIError\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "class VisionManager:\n",
    "    \"\"\"\n",
    "    Dynamic Vision Manager - Optimized for 6GB VRAM\n",
    "    \n",
    "    Models are loaded on-demand and cleared when switching modes.\n",
    "    This allows working with limited VRAM.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, openai_api_key=None):\n",
    "        \"\"\"\n",
    "        Initialize VisionManager\n",
    "        \n",
    "        Args:\n",
    "            openai_api_key: Optional OpenAI API key for premium mode\n",
    "        \"\"\"\n",
    "        self.moondream_model = None\n",
    "        self.moondream_tokenizer = None\n",
    "        self.llava_model = None\n",
    "        self.llava_processor = None\n",
    "        self.openai_client = None\n",
    "        \n",
    "        # Get API key from parameter or environment\n",
    "        self.openai_api_key = openai_api_key or os.getenv('OPENAI_API_KEY')\n",
    "        \n",
    "        # Track currently loaded model\n",
    "        self.current_model = None\n",
    "        \n",
    "        print(\"üîß VisionManager initialized (Dynamic Loading Mode)\")\n",
    "        print(f\"   Premium Mode: {'‚úÖ Ready' if self.openai_api_key else '‚ùå No API key'}\")\n",
    "        print(\"\\\\nüí° Models will load on-demand to save VRAM\")\n",
    "    \n",
    "    def clear_all_models(self):\n",
    "        \"\"\"Clear all loaded models from VRAM\"\"\"\n",
    "        if self.moondream_model is not None:\n",
    "            del self.moondream_model\n",
    "            del self.moondream_tokenizer\n",
    "            self.moondream_model = None\n",
    "            self.moondream_tokenizer = None\n",
    "            print(\"üßπ Cleared Moondream from VRAM\")\n",
    "        \n",
    "        if self.llava_model is not None:\n",
    "            del self.llava_model\n",
    "            del self.llava_processor\n",
    "            self.llava_model = None\n",
    "            self.llava_processor = None\n",
    "            print(\"üßπ Cleared LLaVA from VRAM\")\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            free_vram = (torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated(0)) / 1024**3\n",
    "            print(f\"üíæ Free VRAM: {free_vram:.2f} GB\")\n",
    "        \n",
    "        self.current_model = None\n",
    "    \n",
    "    def load_moondream(self):\n",
    "        \"\"\"Load Moondream2 model\"\"\"\n",
    "        if self.moondream_model is not None:\n",
    "            print(\"‚úÖ Moondream already loaded\")\n",
    "            return True\n",
    "        \n",
    "        try:\n",
    "            print(\"\\\\nüì• Loading Moondream2...\")\n",
    "            \n",
    "            # Clear other models first\n",
    "            if self.current_model == \"llava\":\n",
    "                print(\"   Clearing LLaVA to make room...\")\n",
    "                if self.llava_model is not None:\n",
    "                    del self.llava_model\n",
    "                    del self.llava_processor\n",
    "                    self.llava_model = None\n",
    "                    self.llava_processor = None\n",
    "                    torch.cuda.empty_cache()\n",
    "            \n",
    "            self.moondream_model = AutoModelForCausalLM.from_pretrained(\n",
    "                \"vikhyatk/moondream2\",\n",
    "                revision=\"2024-08-26\",\n",
    "                trust_remote_code=True,\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map=\"auto\" if torch.cuda.is_available() else \"cpu\"\n",
    "            )\n",
    "            \n",
    "            self.moondream_tokenizer = AutoTokenizer.from_pretrained(\n",
    "                \"vikhyatk/moondream2\",\n",
    "                revision=\"2024-08-26\",\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            \n",
    "            self.current_model = \"moondream\"\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                vram_used = torch.cuda.memory_allocated(0) / 1024**3\n",
    "                print(f\"‚úÖ Moondream loaded - VRAM used: {vram_used:.2f} GB\")\n",
    "            else:\n",
    "                print(\"‚úÖ Moondream loaded on CPU\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading Moondream: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def load_llava(self):\n",
    "        \"\"\"Load LLaVA-1.5 model with 4-bit quantization\"\"\"\n",
    "        if self.llava_model is not None:\n",
    "            print(\"‚úÖ LLaVA already loaded\")\n",
    "            return True\n",
    "        \n",
    "        try:\n",
    "            print(\"\\\\nüì• Loading LLaVA-1.5 (4-bit quantization)...\")\n",
    "            \n",
    "            # Clear other models first\n",
    "            if self.current_model == \"moondream\":\n",
    "                print(\"   Clearing Moondream to make room...\")\n",
    "                if self.moondream_model is not None:\n",
    "                    del self.moondream_model\n",
    "                    del self.moondream_tokenizer\n",
    "                    self.moondream_model = None\n",
    "                    self.moondream_tokenizer = None\n",
    "                    torch.cuda.empty_cache()\n",
    "            \n",
    "            # Check available VRAM\n",
    "            if torch.cuda.is_available():\n",
    "                free_vram = (torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated(0)) / 1024**3\n",
    "                print(f\"   Free VRAM: {free_vram:.2f} GB\")\n",
    "                \n",
    "                if free_vram < 4.0:\n",
    "                    print(\"\\\\n‚ö†Ô∏è Not enough free VRAM for LLaVA (needs ~4GB)\")\n",
    "                    print(\"   Trying with CPU offloading...\")\n",
    "            \n",
    "            # 4-bit quantization config\n",
    "            quantization_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_compute_dtype=torch.float16,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                llm_int8_enable_fp32_cpu_offload=True\n",
    "            )\n",
    "            \n",
    "            self.llava_processor = LlavaNextProcessor.from_pretrained(\n",
    "                \"llava-hf/llava-v1.6-mistral-7b-hf\"\n",
    "            )\n",
    "            \n",
    "            self.llava_model = LlavaNextForConditionalGeneration.from_pretrained(\n",
    "                \"llava-hf/llava-v1.6-mistral-7b-hf\",\n",
    "                quantization_config=quantization_config,\n",
    "                device_map=\"auto\",\n",
    "                torch_dtype=torch.float16,\n",
    "                low_cpu_mem_usage=True\n",
    "            )\n",
    "            \n",
    "            self.current_model = \"llava\"\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                vram_used = torch.cuda.memory_allocated(0) / 1024**3\n",
    "                print(f\"‚úÖ LLaVA loaded - VRAM used: {vram_used:.2f} GB\")\n",
    "            else:\n",
    "                print(\"‚úÖ LLaVA loaded on CPU\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading LLaVA: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return False\n",
    "    \n",
    "    def analyze_image(self, image, mode=\"quick\", yolo_detections=None, ocr_results=None):\n",
    "        \"\"\"\n",
    "        Analyze image with selected mode\n",
    "        \n",
    "        Args:\n",
    "            image: PIL Image or path to image\n",
    "            mode: 'quick', 'standard', or 'premium'\n",
    "            yolo_detections: List of YOLO detections\n",
    "            ocr_results: OCR results\n",
    "        \n",
    "        Returns:\n",
    "            dict: Analysis results\n",
    "        \"\"\"\n",
    "        # Load image if path\n",
    "        if isinstance(image, str):\n",
    "            image = Image.open(image)\n",
    "        \n",
    "        # Build context from YOLO and OCR\n",
    "        context = self._build_context(yolo_detections, ocr_results)\n",
    "        \n",
    "        # Route to appropriate mode\n",
    "        if mode == \"quick\":\n",
    "            return self._analyze_quick(image, context)\n",
    "        elif mode == \"standard\":\n",
    "            return self._analyze_standard(image, context)\n",
    "        elif mode == \"premium\":\n",
    "            return self._analyze_premium(image, context)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid mode: {mode}\")\n",
    "    \n",
    "    def _build_context(self, yolo_detections, ocr_results):\n",
    "        \"\"\"Build context from YOLO and OCR results\"\"\"\n",
    "        context = {}\n",
    "        \n",
    "        if yolo_detections:\n",
    "            labels = [d.get('label', '') for d in yolo_detections]\n",
    "            context['detected_labels'] = ', '.join(labels)\n",
    "        \n",
    "        if ocr_results:\n",
    "            context['ocr_text'] = ocr_results.get('raw_text', '')[:500]\n",
    "            context['brand'] = ocr_results.get('brand')\n",
    "            context['product_name'] = ocr_results.get('product_name')\n",
    "        \n",
    "        return context\n",
    "    \n",
    "    def _analyze_quick(self, image, context):\n",
    "        \"\"\"Quick mode with Moondream2\"\"\"\n",
    "        # Load model if not loaded\n",
    "        if not self.load_moondream():\n",
    "            return {\"error\": \"Failed to load Moondream model\"}\n",
    "        \n",
    "        try:\n",
    "            prompt = \"Analyze this food product. \"\n",
    "            if context.get('detected_labels'):\n",
    "                prompt += f\"Labels detected: {context['detected_labels']}. \"\n",
    "            prompt += \"Provide: category, main ingredients (if visible), and key features.\"\n",
    "            \n",
    "            start_time = time.time()\n",
    "            enc_image = self.moondream_model.encode_image(image)\n",
    "            answer = self.moondream_model.answer_question(enc_image, prompt, self.moondream_tokenizer)\n",
    "            elapsed = time.time() - start_time\n",
    "            \n",
    "            return {\n",
    "                \"mode\": \"quick\",\n",
    "                \"response\": answer,\n",
    "                \"time_seconds\": round(elapsed, 2),\n",
    "                \"context_used\": context\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"Quick mode error: {str(e)}\"}\n",
    "    \n",
    "    def _analyze_standard(self, image, context):\n",
    "        \"\"\"Standard mode with LLaVA-1.5\"\"\"\n",
    "        # Load model if not loaded\n",
    "        if not self.load_llava():\n",
    "            return {\"error\": \"Failed to load LLaVA model\"}\n",
    "        \n",
    "        try:\n",
    "            prompt = \"Analyze this food product in detail. \"\n",
    "            if context.get('detected_labels'):\n",
    "                prompt += f\"Labels detected: {context['detected_labels']}. \"\n",
    "            if context.get('ocr_text'):\n",
    "                prompt += f\"Text visible: {context['ocr_text'][:200]}... \"\n",
    "            prompt += \"Provide: category, ingredients, nutritional highlights, and dietary suitability.\"\n",
    "            \n",
    "            conversation = [{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\"},\n",
    "                    {\"type\": \"text\", \"text\": prompt}\n",
    "                ]\n",
    "            }]\n",
    "            \n",
    "            prompt_text = self.llava_processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "            \n",
    "            start_time = time.time()\n",
    "            inputs = self.llava_processor(images=image, text=prompt_text, return_tensors=\"pt\")\n",
    "            \n",
    "            # Move to GPU only if available and has space\n",
    "            if torch.cuda.is_available():\n",
    "                try:\n",
    "                    inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
    "                except:\n",
    "                    print(\"‚ö†Ô∏è Moving inputs to GPU failed, using CPU\")\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                output = self.llava_model.generate(**inputs, max_new_tokens=500, do_sample=False)\n",
    "            \n",
    "            response = self.llava_processor.decode(output[0], skip_special_tokens=True)\n",
    "            \n",
    "            if \"[/INST]\" in response:\n",
    "                response = response.split(\"[/INST]\")[-1].strip()\n",
    "            \n",
    "            elapsed = time.time() - start_time\n",
    "            \n",
    "            return {\n",
    "                \"mode\": \"standard\",\n",
    "                \"response\": response,\n",
    "                \"time_seconds\": round(elapsed, 2),\n",
    "                \"context_used\": context\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            return {\"error\": f\"Standard mode error: {str(e)}\", \"traceback\": traceback.format_exc()}\n",
    "    \n",
    "    def _analyze_premium(self, image, context):\n",
    "        \"\"\"Premium mode with OpenAI GPT-4o\"\"\"\n",
    "        if not self.openai_api_key:\n",
    "            return {\"error\": \"Premium mode not available - no API key. Check your .env file.\"}\n",
    "        \n",
    "        try:\n",
    "            # Initialize client if needed\n",
    "            if not self.openai_client:\n",
    "                self.openai_client = OpenAI(api_key=self.openai_api_key)\n",
    "            \n",
    "            # Encode image\n",
    "            buffered = io.BytesIO()\n",
    "            image.save(buffered, format=\"JPEG\")\n",
    "            image_data = base64.b64encode(buffered.getvalue()).decode('utf-8')\n",
    "            \n",
    "            prompt = \"Analyze this food product and provide structured information.\\\\n\\\\n\"\n",
    "            if context.get('detected_labels'):\n",
    "                prompt += f\"Labels detected: {context['detected_labels']}\\\\n\"\n",
    "            if context.get('ocr_text'):\n",
    "                prompt += f\"Text visible: {context['ocr_text'][:300]}\\\\n\"\n",
    "            \n",
    "            prompt += \"\"\"\\\\nRespond ONLY with a JSON object (no markdown) with this structure:\n",
    "{\n",
    "  \"category\": \"specific food category\",\n",
    "  \"product_type\": \"brief description\",\n",
    "  \"description\": \"2-3 sentence description\",\n",
    "  \"key_ingredients\": [\"list of main ingredients\"],\n",
    "  \"usage_suggestions\": \"how to use this product\",\n",
    "  \"suitable_for\": [\"dietary types\"]\n",
    "}\"\"\"\n",
    "            \n",
    "            start_time = time.time()\n",
    "            response = self.openai_client.chat.completions.create(\n",
    "                model=\"gpt-4o\",\n",
    "                messages=[{\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": prompt},\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\n",
    "                                \"url\": f\"data:image/jpeg;base64,{image_data}\",\n",
    "                                \"detail\": \"low\"\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                }],\n",
    "                max_tokens=800\n",
    "            )\n",
    "            \n",
    "            elapsed = time.time() - start_time\n",
    "            \n",
    "            response_text = response.choices[0].message.content\n",
    "            response_text = re.sub(r'```json\\\\s*|```\\\\s*', '', response_text).strip()\n",
    "            \n",
    "            json_match = re.search(r'\\\\{.*\\\\}', response_text, re.DOTALL)\n",
    "            \n",
    "            if json_match:\n",
    "                result = json.loads(json_match.group())\n",
    "                return {\n",
    "                    \"mode\": \"premium\",\n",
    "                    \"response\": result,\n",
    "                    \"time_seconds\": round(elapsed, 2),\n",
    "                    \"context_used\": context\n",
    "                }\n",
    "            else:\n",
    "                return {\"error\": \"No valid JSON in response\", \"raw_response\": response_text}\n",
    "            \n",
    "        except APIError as e:\n",
    "            return {\"error\": f\"OpenAI API error: {str(e)}\"}\n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"Premium mode error: {str(e)}\"}\n",
    "    \n",
    "    def get_status(self):\n",
    "        \"\"\"Get current status of all modes\"\"\"\n",
    "        return {\n",
    "            \"quick\": {\n",
    "                \"available\": True,\n",
    "                \"loaded\": self.moondream_model is not None\n",
    "            },\n",
    "            \"standard\": {\n",
    "                \"available\": True,\n",
    "                \"loaded\": self.llava_model is not None\n",
    "            },\n",
    "            \"premium\": {\n",
    "                \"available\": self.openai_api_key is not None,\n",
    "                \"loaded\": self.openai_client is not None\n",
    "            },\n",
    "            \"current_model\": self.current_model\n",
    "        }\n",
    "'''\n",
    "\n",
    "# Save to file\n",
    "with open('vision_manager.py', 'w', encoding='utf-8') as f:\n",
    "    f.write(vision_manager_code)\n",
    "\n",
    "print(\"‚úÖ Exported to vision_manager.py\")\n",
    "print(\"\\nYou can now use it in your Streamlit app:\")\n",
    "print(\"\"\"\n",
    "from vision_manager import VisionManager\n",
    "\n",
    "# Create instance (loads API key from .env automatically)\n",
    "vm = VisionManager()\n",
    "\n",
    "# Analyze image\n",
    "result = vm.analyze_image(image, mode=\"quick\")  # or \"standard\" or \"premium\"\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ‚úÖ Summary\n",
    "\n",
    "### What We Built:\n",
    "1. ‚úÖ Dynamic Vision Manager optimized for 6GB VRAM\n",
    "2. ‚úÖ Models load on-demand and clear automatically\n",
    "3. ‚úÖ API key loads from .env file\n",
    "4. ‚úÖ Three modes: Quick (Moondream), Standard (LLaVA), Premium (OpenAI)\n",
    "\n",
    "### How It Works:\n",
    "- **Quick Mode**: Loads Moondream (~3.5GB VRAM)\n",
    "- **Standard Mode**: Clears Moondream, loads LLaVA (~4GB VRAM)\n",
    "- **Premium Mode**: No VRAM needed (uses OpenAI API)\n",
    "\n",
    "### Next Steps:\n",
    "1. Create `.env` file in your project with: `OPENAI_API_KEY=sk-...`\n",
    "2. Test all three modes with your product images\n",
    "3. Ready for Notebook 2: Database + 10k images processing\n",
    "\n",
    "---\n",
    "\n",
    "**Ready for Notebook 2?** üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ironhack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
